{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/elmimansur/NN-Practices/blob/main/try1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1581ed69-dfd5-4537-a1b4-efbbe17155ba",
      "metadata": {
        "id": "1581ed69-dfd5-4537-a1b4-efbbe17155ba"
      },
      "outputs": [],
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1d463be-055b-4343-8a97-c2120d70bb1c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1d463be-055b-4343-8a97-c2120d70bb1c",
        "outputId": "eb7827c3-39ac-4f86-e468-7f50dc6a13e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "mnist = tf.keras.datasets.mnist\n",
        "\n",
        "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "#Reshape\n",
        "x_train2 = x_train.reshape(60000,784)\n",
        "x_test2 = x_test.reshape(10000,784)\n",
        "\n",
        "#One-hot-encoded versions of the labels\n",
        "y_train2 = keras.utils.to_categorical(y_train, num_classes=10)\n",
        "y_test2 = keras.utils.to_categorical(y_test, num_classes=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14f6ccb8-5743-4295-aea0-3069524e9295",
      "metadata": {
        "id": "14f6ccb8-5743-4295-aea0-3069524e9295"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import math   \n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, lr):\n",
        "        dimension = 100\n",
        "        self.lr = lr\n",
        "        self.losses = []\n",
        "         \n",
        "    def sigmoid(self,x):\n",
        "        return np.array(1.0/(1+ np.exp(-x)), dtype=np.float32)\n",
        "    def d_sigmoid (self,x):\n",
        "        return self.sigmoid(x) * (1.0 - self.sigmoid(x))\n",
        "        \n",
        "    def softmax(self,x):\n",
        "        exps = np.exp(x- x.max())\n",
        "        return exps / np.sum(exps, axis=0)\n",
        "    def d_softmax(self,x):\n",
        "        exps = np.exp(x- x.max())\n",
        "        return exps / np.sum(exps, axis=0) * (1 - exps / np.sum(exps, axis=0))\n",
        "\n",
        "    def feedforward(self,x):\n",
        "        self.x = np.expand_dims(x,axis =1)\n",
        "        self.layer1 =  self.weights1 @ self.x + self.bias1\n",
        "        self.output1 = self.sigmoid(self.layer1)\n",
        "        \n",
        "        self.layer2 =  self.weights2 @ self.output1   + self.bias2\n",
        "        self.output= self.softmax(self.layer2)\n",
        "        #self.output=np.squeeze(self.output)\n",
        "\n",
        "    def backprop(self):\n",
        "        d_weights1 = np.zeros(self.weights1.shape)\n",
        "        d_weights2 = np.zeros(self.weights2.shape)\n",
        "        \n",
        "        d_bias1    = np.zeros(self.bias1.shape)\n",
        "        d_bias2    = np.zeros(self.bias2.shape)\n",
        "        loss = 0\n",
        "        for j in range(len(self.y)):\n",
        "            single_input = self.input[j]\n",
        "            self.feedforward(single_input)\n",
        "            d_loss = (self.output - self.y[j])/len(self.y)\n",
        "            loss+= (self.output - self.y[j])**2/len(self.y)\n",
        "            \n",
        "            delta2 = d_loss.T @ self.d_softmax(self.layer2)\n",
        "            d_weights2 += delta2 @ self.output1.T\n",
        "            d_bias2+= delta2\n",
        "            \n",
        "            dA1 =  self.weights2.T @ d_loss\n",
        "            delta1 = dA1 @ self.d_sigmoid(self.layer1)\n",
        "            d_weights1 += delta1 @  self.x.T \n",
        "            d_bias1+= delta1\n",
        "            \n",
        "            \n",
        "        #update for gradient step\n",
        "        self.weights1 -= self.lr*d_weights1\n",
        "        self.weights2 -= self.lr*d_weights2\n",
        "        \n",
        "        self.bias1    -= self.lr*d_bias1\n",
        "        self.bias2    -= self.lr*d_bias2\n",
        "        \n",
        "        self.loss = loss\n",
        "\n",
        "    def initial(self,x,y,n_iter):\n",
        "        dimension = 100\n",
        "        self.input = x\n",
        "        self.weights1 = np.random.rand(dimension, x.shape[1])\n",
        "        self.weights2 = np.random.rand(10,dimension)\n",
        "        \n",
        "        self.bias1 = np.random.rand(dimension,1)\n",
        "        self.bias2 = np.random.rand(10,1)\n",
        "        \n",
        "        self.y            = y\n",
        "        for j in range(n_iter):\n",
        "            self.backprop()\n",
        "            self.losses.append(self.loss)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        self.feedforward(x)\n",
        "        return self.output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63607213-c9ca-4335-929b-db3b3f72b443",
      "metadata": {
        "id": "63607213-c9ca-4335-929b-db3b3f72b443"
      },
      "outputs": [],
      "source": [
        "nn = NeuralNetwork(lr=0.001)\n",
        "nn.initial(x_train2,y_train2,n_iter=10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad6658cb-2191-456d-9cb1-b988775943df",
      "metadata": {
        "id": "ad6658cb-2191-456d-9cb1-b988775943df",
        "outputId": "8f6816ec-a3f2-42d7-9814-9d06ac21db7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy: 0.098\n"
          ]
        }
      ],
      "source": [
        "predicted = np.zeros(y_test2.shape)\n",
        "for i in range(len(predicted)):\n",
        "    predicted[i] = nn.predict(np.array(x_test2[i]))\n",
        "\n",
        "predicted_class = np.round(predicted)\n",
        "\n",
        "# Calculate correct matches ,, max almak lazım\n",
        "match_count = sum([int((y == y_).all()) for y, y_ in zip(y_test2, predicted_class)])\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = match_count / len(y_test2)\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {:.3f}\".format(accuracy))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1dee680-558a-442c-9377-87829c3fbe53",
      "metadata": {
        "id": "f1dee680-558a-442c-9377-87829c3fbe53"
      },
      "outputs": [],
      "source": [
        "## dersteki örnek kod: "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import normalize\n",
        "import math\n",
        "\n",
        "def sigmoid(x):\n",
        "    return np.array(1.0/(1+ np.exp(-x)), dtype=np.float32)\n",
        "    \n",
        "def sigmoid_derivative(x):\n",
        "    return sigmoid(x) * (1.0 - sigmoid(x))\n",
        "\n",
        "class NeuralNetwork:\n",
        "    def __init__(self, lr):\n",
        "        dimension = 1\n",
        "        self.lr = lr\n",
        "        self.activation   = sigmoid\n",
        "        self.d_activation = sigmoid_derivative\n",
        "        self.losses = []\n",
        "\n",
        "    def feedforward(self,x):\n",
        "        self.x = np.expand_dims(x,axis =1)\n",
        "        self.layer1_pa = self.weights1 @  self.x + self.bias1\n",
        "        self.output = self.activation(self.layer1_pa)\n",
        "\n",
        "    def backprop(self):\n",
        "        # application of the chain rule to find derivative of the loss function with respect to weights2 and weights1\n",
        "        d_weights1 = np.zeros(self.weights1.shape)\n",
        "        d_bias1    = np.zeros(self.bias1.shape)\n",
        "        loss = 0\n",
        "        for j in range(len(self.y)):\n",
        "            # compute gradient per each input image\n",
        "            single_input = self.input[j]\n",
        "            self.feedforward(single_input)\n",
        "            d_loss = (self.output - self.y[j])/len(self.y)\n",
        "\n",
        "            #softmax'i loss'a /binary cross\n",
        "            loss+= (self.output - self.y[j])**2/len(self.y)#(self.y[j]*self.output + (1-self.y[j])*(1-self.output))**2/len(self.y)\n",
        "\n",
        "            delta = d_loss @ self.d_activation(self.layer1_pa)\n",
        "\n",
        "            d_weights1 +=  delta @ self.x.T\n",
        "            d_bias1    += delta\n",
        "        #taking the gradient step with learning rate lr per point\n",
        "        self.weights1 -= self.lr*d_weights1\n",
        "        self.bias1    -= self.lr*d_bias1\n",
        "        self.loss = loss\n",
        "\n",
        "    def fit(self,x,y,n_iter):\n",
        "        dimension = 1\n",
        "        self.input        = x\n",
        "        self.weights1     = np.random.rand(dimension, x.shape[1])\n",
        "        self.bias1        = np.random.rand(dimension,1)      \n",
        "        self.y            = y\n",
        "        for j in range(n_iter):\n",
        "            self.backprop()\n",
        "            self.losses.append(self.loss)\n",
        "    \n",
        "    def predict(self,x):\n",
        "        self.feedforward(x)\n",
        "        return self.output"
      ],
      "metadata": {
        "id": "90lc5YKlQsEH"
      },
      "id": "90lc5YKlQsEH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nn = NeuralNetwork(lr=0.001)\n",
        "nn.fit(x_train2,y_train2,n_iter=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "tYMpHpcbZEeG",
        "outputId": "89fe7e3d-4ed9-4332-d9b1-3b1d1b4e50ae"
      },
      "id": "tYMpHpcbZEeG",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-cd8f08c8a25e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNeuralNetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-83ca2440ae07>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, n_iter)\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-83ca2440ae07>\u001b[0m in \u001b[0;36mbackprop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#(self.y[j]*self.output + (1-self.y[j])*(1-self.output))**2/len(self.y)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_loss\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0md_activation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1_pa\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0md_weights1\u001b[0m \u001b[0;34m+=\u001b[0m  \u001b[0mdelta\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: matmul: Input operand 1 has a mismatch in its core dimension 0, with gufunc signature (n?,k),(k,m?)->(n?,m?) (size 1 is different from 10)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "oWu9w_nmZJYw"
      },
      "id": "oWu9w_nmZJYw",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.11"
    },
    "colab": {
      "name": "Question b.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}